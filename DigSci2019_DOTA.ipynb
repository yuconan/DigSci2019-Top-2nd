{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DigSci2019 科学数据挖掘大赛   Final Top-2\n",
    "###  作者 : DOTA  \n",
    "#### 分数( MAP@3 ):0.53733 (Rank2) \n",
    "\n",
    "**方法** :   \n",
    ">在解决本问题时，我借鉴了推荐算法的思想，将问题拆解了两部分——召回和排序。在召回阶段，使用了两种方式，其一是利用Wrod2Vec和TFIDF方法，将描述段落利用Word2Vec得到每个词的词向量，同时对句子中的词使用TF-IDF为权重进行加权得到Sentence Embedding，同时为了得到更好的效果，这里做了一个改进，即使用Smooth Inverse Frequency代替TFIDF作为每个词的权重；其二是利用TFIDF得到Sentence Embedding。两种方法各自计算余弦相似度得到3篇论文，去重后召回集中每个段落有3-6篇不等的召回论文。  \n",
    "在排序阶段，我们利用BERT对描述段落Description和论文文本PaperText组成句子对（Description，PaperText）进行编码，在输出层经过Dense和Softmax层后得到概率值后排序。\n",
    ">\n",
    "**模型** : Word2Vec,TF-IDF,BERT  \n",
    "**测试环境** : Ubuntu18,CPU32核,内存64G,两块显卡RTX2080Ti  \n",
    "\n",
    "**模型说明** :\n",
    ">从任务描述中我们可以看到，该任务需要对描述段落匹配三篇最相关的论文。单从形式上可以理解为这是一个“完形填空”任务。但相较于在本文的相应位置上填上相应的词语不同的是，这里需要填充的是一个Sentence，也就是论文的题目。但是如果你按照这个思路去寻求解决方案，你会发现在这个量级的文本数据上，一般算力是满足不了的。\n",
    "既然如此，那我们不如换一个思路来思考这个问题，“对描述段落匹配三篇最相关的论文”，其实最简单的实现方式是计算描述段落和论文库里所有论文的相似度，找出最相似的即可。但这同样会存在一个问题，通过对数据进行探查你会发现“An efficient implementation based on BERT [1] and graph neural network (GNN) [2] is introduced.”这一描述段落，同时引用了两篇文章，那么在计算相似度时，到底哪个位置该是哪篇文章呢？\n",
    "\n",
    "**代码说明** :  \n",
    "**1、RecallPart**：两种方法各自计算余弦相似度得到3篇论文，去重后召回集中每个段落有3-6篇不等的召回论文；  \n",
    "**1.1 ProSolution1** : 利用TF-IDF计算相似度召回3篇论文；   \n",
    "**1.2 ProSolution2** : 利用DOTA-EmbeddingVector计算相似度召回3篇论文  \n",
    "**2、SortPart**：利用BERT利用Encoder描述段落和候选论文，计算相似度；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecallPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/train_release.csv\")\n",
    "valid_data = pd.read_csv(\"../data/validation_release.csv\")\n",
    "paper_data = pd.read_csv(\"../data/candidate_paper.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_nums = 3\n",
    "run_nums = 1000\n",
    "\n",
    "def find_candidate(x):\n",
    "    cand_pos = x.find(\"[**##**]\")\n",
    "    cand_lst = x[:cand_pos].split(\".\")\n",
    "    if len(cand_lst[-1]) < 20 :\n",
    "        sp = \" \".join(cand_lst[-2:])\n",
    "    else:\n",
    "        sp =  cand_lst[-1]\n",
    "        \n",
    "    return x.replace(\"[**##**]\",sp)\n",
    "\n",
    "paper_data['description_text'] = paper_data.title.fillna(\" \")\n",
    "valid_data[\"description\"] = valid_data[\"description_text\"].map(lambda x:find_candidate(str(x)))\n",
    "train_data[\"description\"] = train_data[\"description_text\"].map(lambda x:find_candidate(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_enc = TfidfVectorizer(ngram_range=(1, 4),min_df=5,max_df=0.9)\n",
    "tfidf_enc.fit(list(train_data.description_text.fillna(\" \")) + list(valid_data.description_text.fillna(\" \")) + list(paper_data.description_text.fillna(\" \")))\n",
    "\n",
    "train_mat = tfidf_enc.transform(train_data.description.fillna(\"No Description\"))\n",
    "valid_mat = tfidf_enc.transform(valid_data.description.fillna(\"No Description\"))\n",
    "paper_mat = tfidf_enc.transform(paper_data.description_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PS1-GetTrainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame()\n",
    "\n",
    "head_nums = int(train_mat.shape[0]/run_nums)\n",
    "tail_nums = train_mat.shape[0] - head_nums * run_nums\n",
    "\n",
    "for i in tqdm(range(head_nums)):\n",
    "    i *= run_nums\n",
    "    paperid = np.argsort(np.dot(train_mat[i:i+run_nums],paper_mat.T).todense())[:,-recall_nums:]\n",
    "    discrid = train_data.description_id.values[i:i+run_nums]\n",
    "    df_tmp = pd.DataFrame(paperid,columns=[\"paper_\"+str(recall_nums-j) for j in range(recall_nums)])\n",
    "    df_tmp[\"description_id\"] = discrid\n",
    "    train_df = pd.concat([train_df,df_tmp],axis=0,sort=False)\n",
    "\n",
    "# tail\n",
    "paperid = np.argsort(np.dot(train_mat[-tail_nums:],paper_mat.T).todense())[:,-recall_nums:]\n",
    "discrid = train_data.description_id.values[-tail_nums:]\n",
    "df_tmp = pd.DataFrame(paperid,columns=[\"paper_\"+str(recall_nums-j) for j in range(recall_nums)])\n",
    "df_tmp[\"description_id\"] = discrid\n",
    "train_df = pd.concat([train_df,df_tmp],axis=0,sort=False)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS1-GetValidX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_df = pd.DataFrame()\n",
    "\n",
    "head_nums = int(valid_mat.shape[0]/run_nums)\n",
    "tail_nums = valid_mat.shape[0] - head_nums * run_nums\n",
    "\n",
    "for i in tqdm(range(head_nums)):\n",
    "    i *= run_nums\n",
    "    paperid = np.argsort(np.dot(valid_mat[i:i+run_nums],paper_mat.T).todense())[:,-recall_nums:]\n",
    "    discrid = valid_data.description_id.values[i:i+run_nums]\n",
    "    df_tmp = pd.DataFrame(paperid,columns=[\"paper_\"+str(recall_nums-j) for j in range(recall_nums)])\n",
    "    df_tmp[\"description_id\"] = discrid\n",
    "    valid_df = pd.concat([valid_df,df_tmp],axis=0,sort=False)\n",
    "\n",
    "# tail\n",
    "paperid = np.argsort(np.dot(valid_mat[-tail_nums:],paper_mat.T).todense())[:,-recall_nums:]\n",
    "discrid = valid_data.description_id.values[-tail_nums:]\n",
    "df_tmp = pd.DataFrame(paperid,columns=[\"paper_\"+str(recall_nums-j) for j in range(recall_nums)])\n",
    "df_tmp[\"description_id\"] = discrid\n",
    "valid_df = pd.concat([valid_df,df_tmp],axis=0,sort=False)\n",
    "print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_paperid = dict(zip(paper_data[['paper_id']].reset_index()[\"index\"].values,paper_data[['paper_id']].reset_index()[\"paper_id\"].values))\n",
    "\n",
    "for i in range(1,recall_nums+1):\n",
    "    train_df['paper_'+str(i)] = train_df['paper_'+str(i)].map(lambda x: dict_paperid[x])\n",
    "\n",
    "for i in range(1,recall_nums+1):\n",
    "    valid_df['paper_'+str(i)] = valid_df['paper_'+str(i)].map(lambda x: dict_paperid[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.DataFrame()\n",
    "valid_set = pd.DataFrame()\n",
    "\n",
    "for i in range(1,1+recall_nums):\n",
    "    tmp = train_df[[\"description_id\",\"paper_\"+str(i)]]\n",
    "    tmp.columns = [\"description_id\",\"paper_id\"]\n",
    "    train_set = pd.concat([train_set,tmp],axis=0,sort=False)\n",
    "\n",
    "for i in range(1,1+recall_nums):\n",
    "    tmp = valid_df[[\"description_id\",\"paper_\"+str(i)]]\n",
    "    tmp.columns = [\"description_id\",\"paper_id\"]\n",
    "    valid_set = pd.concat([valid_set,tmp],axis=0,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS1-SaveSetX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"label\"] = 1\n",
    "train_set = train_set.merge(train_data[[\"description_id\",\"paper_id\",\"label\"]], how ='left',on =[\"description_id\",\"paper_id\"]).fillna(0)\n",
    "tmp = train_set.groupby(\"description_id\",as_index=False)[\"label\"].agg({\"score\":\"sum\"})\n",
    "tmp = tmp[tmp.score > 0]\n",
    "train_set = tmp[[\"description_id\"]].merge(train_set,how='left',on='description_id')\n",
    "train_set.to_csv(\"../data/train_set_x.csv\",index=False)\n",
    "valid_set.to_csv(\"../data/valid_set_x.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS2-DOTA embedding Vecter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "modelW2V = Word2Vec(sentences=list(train_data.description_text.fillna(\" \")) + list(paper_data.description_text.fillna(\" \")), size=100, seed=2019)\n",
    "\n",
    "w2v = {w: vec for w, vec in zip(modelW2V.wv.index2word, modelW2V.wv.syn0)}\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class DOTAembeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(word2vec)))\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "\n",
    "dota_enc = DOTAembeddingVectorizer(w2v)\n",
    "dota_enc.fit(list(train_data.description_text.fillna(\" \")) + list(paper_data.description_text.fillna(\" \")))\n",
    "\n",
    "train_mat = dota_enc.transform(train_data.description.fillna(\"No Description\"))\n",
    "valid_mat = dota_enc.transform(valid_data.description.fillna(\"No Description\"))\n",
    "paper_mat = dota_enc.transform(paper_data.description_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS2-GetTrainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame()\n",
    "\n",
    "head_nums = int(train_mat.shape[0]/run_nums)\n",
    "tail_nums = train_mat.shape[0] - head_nums * run_nums\n",
    "\n",
    "for i in tqdm(range(head_nums)):\n",
    "    i *= run_nums\n",
    "    paperid = np.argsort(np.dot(train_mat[i:i+run_nums],paper_mat.T).todense())[:,-recall_nums:]\n",
    "    discrid = train_data.description_id.values[i:i+run_nums]\n",
    "    df_tmp = pd.DataFrame(paperid,columns=[\"paper_\"+str(recall_nums-j) for j in range(recall_nums)])\n",
    "    df_tmp[\"description_id\"] = discrid\n",
    "    train_df = pd.concat([train_df,df_tmp],axis=0,sort=False)\n",
    "\n",
    "# tail\n",
    "paperid = np.argsort(np.dot(train_mat[-tail_nums:],paper_mat.T).todense())[:,-recall_nums:]\n",
    "discrid = train_data.description_id.values[-tail_nums:]\n",
    "df_tmp = pd.DataFrame(paperid,columns=[\"paper_\"+str(recall_nums-j) for j in range(recall_nums)])\n",
    "df_tmp[\"description_id\"] = discrid\n",
    "train_df = pd.concat([train_df,df_tmp],axis=0,sort=False)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS2-GetValidY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.DataFrame()\n",
    "\n",
    "head_nums = int(valid_mat.shape[0]/run_nums)\n",
    "tail_nums = valid_mat.shape[0] - head_nums * run_nums\n",
    "\n",
    "for i in tqdm(range(head_nums)):\n",
    "    i *= run_nums\n",
    "    paperid = np.argsort(np.dot(valid_mat[i:i+run_nums],paper_mat.T).todense())[:,-recall_nums:]\n",
    "    discrid = valid_data.description_id.values[i:i+run_nums]\n",
    "    df_tmp = pd.DataFrame(paperid,columns=[\"paper_\"+str(recall_nums-j) for j in range(recall_nums)])\n",
    "    df_tmp[\"description_id\"] = discrid\n",
    "    valid_df = pd.concat([valid_df,df_tmp],axis=0,sort=False)\n",
    "\n",
    "# tail\n",
    "paperid = np.argsort(np.dot(valid_mat[-tail_nums:],paper_mat.T).todense())[:,-recall_nums:]\n",
    "discrid = valid_data.description_id.values[-tail_nums:]\n",
    "df_tmp = pd.DataFrame(paperid,columns=[\"paper_\"+str(recall_nums-j) for j in range(recall_nums)])\n",
    "df_tmp[\"description_id\"] = discrid\n",
    "valid_df = pd.concat([valid_df,df_tmp],axis=0,sort=False)\n",
    "print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_paperid = dict(zip(paper_data[['paper_id']].reset_index()[\"index\"].values,paper_data[['paper_id']].reset_index()[\"paper_id\"].values))\n",
    "\n",
    "for i in range(1,recall_nums+1):\n",
    "    train_df['paper_'+str(i)] = train_df['paper_'+str(i)].map(lambda x: dict_paperid[x])\n",
    "\n",
    "for i in range(1,recall_nums+1):\n",
    "    valid_df['paper_'+str(i)] = valid_df['paper_'+str(i)].map(lambda x: dict_paperid[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.DataFrame()\n",
    "valid_set = pd.DataFrame()\n",
    "\n",
    "for i in range(1,1+recall_nums):\n",
    "    tmp = train_df[[\"description_id\",\"paper_\"+str(i)]]\n",
    "    tmp.columns = [\"description_id\",\"paper_id\"]\n",
    "    train_set = pd.concat([train_set,tmp],axis=0,sort=False)\n",
    "\n",
    "for i in range(1,1+recall_nums):\n",
    "    tmp = valid_df[[\"description_id\",\"paper_\"+str(i)]]\n",
    "    tmp.columns = [\"description_id\",\"paper_id\"]\n",
    "    valid_set = pd.concat([valid_set,tmp],axis=0,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS2-SaveSetY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"label\"] = 1\n",
    "train_set = train_set.merge(train_data[[\"description_id\",\"paper_id\",\"label\"]], how ='left',on =[\"description_id\",\"paper_id\"]).fillna(0)\n",
    "\n",
    "tmp = train_set.groupby(\"description_id\",as_index=False)[\"label\"].agg({\"score\":\"sum\"})\n",
    "tmp = tmp[tmp.score > 0]\n",
    "train_set = tmp[[\"description_id\"]].merge(train_set,how='left',on='description_id')\n",
    "\n",
    "train_set.to_csv(\"../data/train_set_y.csv\",index=False)\n",
    "valid_set.to_csv(\"../data/valid_set_y.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SortPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_setx = pd.read_csv(\"../data/train_set_x.csv\")\n",
    "test_setx = pd.read_csv(\"../data/valid_set_x.csv\")\n",
    "train_sety = pd.read_csv(\"../data/train_set_y.csv\")\n",
    "test_sety = pd.read_csv(\"../data/valid_set_y.csv\")\n",
    "\n",
    "train_set = pd.concat([train_setx,train_sety],axis=0,sort=False).dorp_duplicates()\n",
    "test_set  = pd.concat([test_setx,test_setu],axis=0,sort=False).dorp_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_data = pd.read_csv(\"../data/candidate_paper.csv\")\n",
    "train_data = pd.read_csv(\"../data/train_release.csv\")\n",
    "test_data  = pd.read_csv(\"../data/validation_release.csv\")\n",
    "\n",
    "paper_data[\"paper_text\"] = paper_data.title.fillna(\" \")\n",
    "\n",
    "def find_candidate(x):\n",
    "    cand_pos = x.find(\"[**##**]\")\n",
    "    cand_lst = x[:cand_pos].split(\".\")\n",
    "    if len(cand_lst[-1]) < 20 :\n",
    "        sp = \" \".join(cand_lst[-2:])\n",
    "    else:\n",
    "        sp =  cand_lst[-1]\n",
    "        \n",
    "    return x.replace(\"[**##**]\",sp)\n",
    "\n",
    "train_data[\"description_text\"] = train_data[\"description_text\"].map(lambda x : find_candidate(str(x)))\n",
    "test_data[\"description_text\"]  = test_data[\"description_text\"].map(lambda x : find_candidate(str(x)))\n",
    "\n",
    "paper_data[\"paper_text\"] = paper_data[\"paper_text\"].map(lambda x : str(x).lower())\n",
    "\n",
    "# conc data\n",
    "train = train_set.merge(train_data[[\"description_id\",\"description_text\"]],how='left',on='description_id')\n",
    "train = train.merge(paper_data[[\"paper_id\",\"paper_text\"]],how='left',on='paper_id')\n",
    "\n",
    "del train_set,train_data\n",
    "gc.collect()\n",
    "\n",
    "test = test_set.merge(test_data[[\"description_id\",\"description_text\"]],how='left',on='description_id')\n",
    "test = test.merge(paper_data[[\"paper_id\",\"paper_text\"]],how='left',on='paper_id')\n",
    "\n",
    "del test_set,test_data,paper_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"description_text\"] = train[\"description_text\"].astype(str).fillna(\" \")\n",
    "train[\"paper_text\"] = train[\"paper_text\"].astype(str).fillna(\" \")\n",
    "\n",
    "test[\"description_text\"] = train[\"description_text\"].astype(str).fillna(\" \")\n",
    "test[\"paper_text\"] = train[\"paper_text\"].astype(str).fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"description\"] = train[\"description_text\"] + train[\"paper_text\"]\n",
    "test[\"description\"] = test[\"description_text\"] + train[\"paper_text\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelBERT2Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "min_learning_rate = 1e-5\n",
    "config_path = '../uncased_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = '../uncased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = '../uncased_L-12_H-768_A-12/vocab.txt'\n",
    "MAX_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with open(dict_path, 'r', encoding='utf-8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "save_path = \"../model/bert_epoch{0}/\".format(epochs)\n",
    "if not os.path.exists(save_path):    \n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "if not os.path.exists(save_path+\"submission/\"):    \n",
    "    os.mkdir(save_path+\"submission/\")    \n",
    "    \n",
    "if not os.path.exists(save_path+\"log/\"):    \n",
    "    os.mkdir(save_path+\"log/\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = save_path+\"log/\"\n",
    "# 创建一个logger\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建一个handler，\n",
    "timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n",
    "fh = logging.FileHandler(file_path + 'log_' + timestamp +'.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "\n",
    "# 再创建一个handler，用于输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# 定义handler的输出格式\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# 给logger添加handler\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "def read_data(file_path, id, name):\n",
    "    train_id = []\n",
    "    train_title = []\n",
    "    train_text = []\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line = line.strip().split(',')\n",
    "            train_id.append(line[0].replace('\\'', '').replace(' ', ''))\n",
    "            train_title.append(line[1])\n",
    "            train_text.append('，'.join(line[2:]))\n",
    "    output = pd.DataFrame(dtype=str)\n",
    "    output[id] = train_id\n",
    "    output[name + '_title'] = train_title\n",
    "    output[name + '_content'] = train_text\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_achievements = train['description_text'].values\n",
    "train_requirements = train['title'].values\n",
    "\n",
    "labels = train['label'].astype(int).values \n",
    "labels_cat = to_categorical(labels)\n",
    "labels_cat = labels_cat.astype(np.int32)\n",
    "\n",
    "test_achievements = test['description_text'].values\n",
    "test_requirements = test['title'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=64):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data[0]) // self.batch_size\n",
    "        if len(self.data[0]) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            X1, X2, y = self.data\n",
    "            idxs = list(range(len(self.data[0])))\n",
    "            np.random.shuffle(idxs)\n",
    "            T, T_, Y = [], [], []\n",
    "            for c, i in enumerate(idxs):\n",
    "                achievements = str(X1[i])[:300]\n",
    "                requirements = str(X2[i])[:30]\n",
    "                t, t_ = tokenizer.encode(first=achievements, second=requirements, max_len=330)\n",
    "                T.append(t)\n",
    "                T_.append(t_)\n",
    "                Y.append(y[i])\n",
    "                if len(T) == self.batch_size or i == idxs[-1]:\n",
    "                    T = np.array(T)\n",
    "                    T_ = np.array(T_)\n",
    "                    Y = np.array(Y)\n",
    "                    yield [T, T_], Y\n",
    "                    T, T_, Y = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "\n",
    "    T1 = Input(shape=(None,))\n",
    "    T2 = Input(shape=(None,))\n",
    "\n",
    "    T = bert_model([T1, T2])\n",
    "\n",
    "    T = Lambda(lambda x: x[:, 0])(T)\n",
    "\n",
    "    output = Dense(2, activation='softmax')(T)\n",
    "\n",
    "    model = Model([T1, T2], output)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(1e-5),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(Callback):\n",
    "    def __init__(self, val_data, val_index,model_path):\n",
    "        self.score = []\n",
    "        self.best = 0.\n",
    "        self.early_stopping = 0\n",
    "        self.val_data = val_data\n",
    "        self.val_index = val_index\n",
    "        self.predict = []\n",
    "        self.lr = 0\n",
    "        self.passed = 0\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.passed < self.params['steps']:\n",
    "            self.lr = (self.passed + 1.) / self.params['steps'] * learning_rate\n",
    "            K.set_value(self.model.optimizer.lr, self.lr)\n",
    "            self.passed += 1\n",
    "        elif self.params['steps'] <= self.passed < self.params['steps'] * 2:\n",
    "            self.lr = (2 - (self.passed + 1.) / self.params['steps']) * (learning_rate - min_learning_rate)\n",
    "            self.lr += min_learning_rate\n",
    "            K.set_value(self.model.optimizer.lr, self.lr)\n",
    "            self.passed += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        score, acc, f1 = self.evaluate()\n",
    "        if acc > self.best:\n",
    "            self.best = acc\n",
    "            self.early_stopping = 0\n",
    "            model.save_weights(self.model_path)\n",
    "        else:\n",
    "            self.early_stopping += 1\n",
    "        logger.info('lr: %.6f, epoch: %d, score: %.4f, acc: %.4f, f1: %.4f,best: %.4f\\n' % (self.lr, epoch, score, acc, f1, self.best))\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.predict = []\n",
    "        prob = []\n",
    "        val_x1, val_x2, val_y, val_cat = self.val_data\n",
    "        for i in tqdm(range(len(val_x1))):\n",
    "            achievements = str(val_x1[i])[:300]\n",
    "            requirements = str(val_x2[i])[:30]\n",
    "\n",
    "            t1, t1_ = tokenizer.encode(first=achievements, second=requirements)\n",
    "            T1, T1_ = np.array([t1]), np.array([t1_])\n",
    "            _prob = model.predict([T1, T1_])\n",
    "            oof_train[self.val_index[i]] = _prob[0]\n",
    "            self.predict.append(np.argmax(_prob, axis=1)[0])\n",
    "            prob.append(_prob[0])\n",
    "\n",
    "        score = 1.0 / (1 + mean_absolute_error(val_y, self.predict))\n",
    "        acc = accuracy_score(val_y, self.predict)\n",
    "        f1 = f1_score(val_y, self.predict, average='macro')\n",
    "        return score, acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predict_generator:\n",
    "    def __init__(self, data, batch_size=256):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data[0]) // self.batch_size\n",
    "        if len(self.data[0]) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            X1, X2 = self.data\n",
    "            idxs = list(range(len(self.data[0])))\n",
    "            T, T_, = [], []\n",
    "            for c, i in enumerate(idxs):\n",
    "                achievements = str(X1[i])[:300]\n",
    "                requirements = str(X2[i])[:30]\n",
    "                t, t_ = tokenizer.encode(first=achievements, second=requirements, max_len=330)\n",
    "                T.append(t)\n",
    "                T_.append(t_)\n",
    "                if len(T) == self.batch_size or i == idxs[-1]:\n",
    "                    T = np.array(T)\n",
    "                    T_ = np.array(T_)\n",
    "                    yield [T, T_]\n",
    "                    T, T_ = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_train = np.zeros((len(train), 2), dtype=np.float32)\n",
    "oof_test = np.zeros((len(test), 2), dtype=np.float32)\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(train_achievements, labels)):\n",
    "    logger.info('================     fold {}        ==============='.format(fold))\n",
    "    x1 = train_achievements[train_index]\n",
    "    x2 = train_requirements[train_index]\n",
    "    y = labels_cat[train_index]\n",
    "\n",
    "    val_x1 = train_achievements[valid_index]\n",
    "    val_x2 = train_requirements[valid_index]\n",
    "    val_y = labels[valid_index]\n",
    "    val_cat = labels_cat[valid_index]\n",
    "\n",
    "    train_D = data_generator([x1, x2, y])\n",
    "    model_save_path = save_path + \"BERTModel_{0}.weights\".format(str(fold))\n",
    "    evaluator = Evaluate([val_x1, val_x2, val_y, val_cat], valid_index,model_save_path)\n",
    "\n",
    "    model = get_model()\n",
    "    \n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                        steps_per_epoch=len(train_D),\n",
    "                        epochs=epochs,\n",
    "                        callbacks=[evaluator]\n",
    "                       )\n",
    "    model.load_weights(model_save_path)\n",
    "    \n",
    "    test_D = predict_generator([test_achievements, test_requirements])\n",
    "    oof_test += model.predict_generator(test_D.__iter__(), steps=len(test_D))\n",
    "    print(oof_test)\n",
    "    break\n",
    "    K.clear_session()\n",
    "oof_test /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = test[['description_id','paper_id']]\n",
    "submit['proba'] = oof_test\n",
    "submit[\"ranks\"] = submit[\"proba\"].groupby(submit[\"description_id\"]).rank(ascending=0,method='first')\n",
    "submit.sort_values(by='description_id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = submit[submit.ranks == 1][[\"description_id\",\"paper_id\"]]\n",
    "sub1.columns = [\"description_id\",\"paper_1\"]\n",
    "sub2 = submit[submit.ranks == 2][[\"description_id\",\"paper_id\"]]\n",
    "sub2.columns = [\"description_id\",\"paper_2\"]\n",
    "sub3 = submit[submit.ranks == 3][[\"description_id\",\"paper_id\"]]\n",
    "sub3.columns = [\"description_id\",\"paper_3\"]\n",
    "result = sub1.merge(sub2, how = 'left', on = 'description_id')\n",
    "result = result.merge(sub3, how = 'left', on = 'description_id')\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[['description_id','paper_1','paper_2','paper_3']].to_csv(\"submit.tsv\",index=False,header=None,sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
